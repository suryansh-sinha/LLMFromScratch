{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4e9183",
   "metadata": {},
   "source": [
    "### 1. How do you prepare input text for training LLMs?\n",
    "\n",
    "- Step 1: Splitting text into individual words and subwords\n",
    "- Step 2: Convert tokens into token IDs\n",
    "- Step 3: Encode token IDs into vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce87279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)    # splits before each whitespace character\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ba6785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# We want the punctution as separate tokens too.\n",
    "result = re.split(r'([,.]|\\s)', text)  # splits on commas, periods, and whitespace\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90cfa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Let's remove the white spaces\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151ba83",
   "metadata": {},
   "source": [
    "Removing whitespaces is a choice that we have to make. It reduces the memory and computing requirements. But if the task that you want to perform is sensitive to white spaces, for example Python code generation, you may want to keep them. For now, we are removing white spaces for simplicity but later, we'll switch to a tokenization scheme that includes white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b87437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11993f1b",
   "metadata": {},
   "source": [
    "Since we've got a basic tokenizer, let's now apply this to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c763b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032c3f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e624b52",
   "metadata": {},
   "source": [
    "### Converting Tokens into Token IDs\n",
    "We need to convert each token into a token ID so that we can represent them numerically. First, we need to build a vocabulary for our dataset. The tokens are sorted and then each unique token in the vocabulary is mapped to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7447ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))  # remove duplicates and sort\n",
    "vocab_size = len(all_words)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412218c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a4d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f77423",
   "metadata": {},
   "source": [
    "Later, when we want to convert from token IDs to text, we will have a decoder (reverse of the above function) that maps token IDs to the text from vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3eb4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # remove space before punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c45bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "Encoded: [53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709]\n",
      "Decoded: I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "encoded = tokenizer.encode(raw_text[:99])\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(raw_text[:99])\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142b8b8",
   "metadata": {},
   "source": [
    "### Adding special context tokens\n",
    "\n",
    "In this section, we will modify the tokenizer to handle unknown words i.e. words that are not present in our vocabulary.\n",
    "\n",
    "We add <|unk|> and <|endoftext|> tokens to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00cec8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de89de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e124bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031cf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] if s in self.str_to_int else self.str_to_int[\"<|unk|>\"] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f018c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4a256e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fcd88a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e91c0",
   "metadata": {},
   "source": [
    "### Some commonly used Special Context Tokens\n",
    "- [BOS](beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "- [EOS](end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts.\n",
    "- [PAD](padding token): When training LLMs with batch size of more than one, the batch might contain sequences of unequal lengths. The shorter texts are extended or \"padded\" up to the length of the longest sequence. This token is ignored by the LLM, it's just there to keep sequence length equal.\n",
    "\n",
    "The tokenizer used by GPT models does not need any of these tokens. It only needs the <|endoftext|> token.\n",
    "\n",
    "The tokenizer used for GPT models doesn't use an <|unk|> token for out-of-vocab words. Instead, GPT models use byte-pair encoding tokenizer, which breaks down words into subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c064d5",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding Tokenizer\n",
    "\n",
    "BPE is a sub-word tokenization algorithm.\n",
    "\n",
    "There are 3 kinds of tokenizers:\n",
    "1. Word Tokenizer: Each word is a token. This is simple but not efficient for large vocabularies. The problem with this is that what do we do with out of vocabulary (OOV) words? Another problem is what do we do when same words are used in different contexts? For example, \"bank\" can mean a financial institution or the side of a river. This tokenizer does not handle these cases well.\n",
    "2. Subword Tokenizer: Words are broken down into smaller subword units. Subword splitting helps the model learn that different words with the same root word as \"token\" like \"tokens\" and \"tokenizing\" are similar in meaning. It also helps the model learn that \"tokenization\" and \"modernization\" are made up of different root words but have the same suffix \"ization\" and are used in same syntactic contexts. It has two rules:\n",
    "    - Do not split frequently used words into smaller subwords.\n",
    "    - Split the rare words into smaller, meaningful subwords.\n",
    "    - For example, \"boy\" should not be split but \"boys\" should be split into \"boy\" and \"s\"\n",
    "3. Character Tokenizer: Each character is a token. It has a very small vocabulary since the English language has 256 characters; this solves the OOV problem. The problem that we have with this is that the tokenized sequence is much longer than the initial raw text. Another problem is that the meaning of the words is lost. For example, \"bat\" can be a flying mammal or a piece of sports equipment. The character tokenizer does not capture this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c2c5c",
   "metadata": {},
   "source": [
    "### The BPE Algorithm\n",
    "\n",
    "BPE Algorithm (1994): Most common pair of consecutive bytes of data is replaced with a byte of data that does not occur in data. This wikipedia article has a good explanation of the algorithm: https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
    "\n",
    "Example: Let's assume that our original data is aaabdaaabac\n",
    "\n",
    "1. Count the frequency of each pair of consecutive bytes:\n",
    "   - aa: 4\n",
    "   - ab: 2\n",
    "   - ba: 1\n",
    "   - ac: 1\n",
    "2. Find the most frequent pair, which is \"aa\" in this case.\n",
    "3. Replace the most frequent pair with a new byte, say \"Z\" since it's not occuring in our data. The new data becomes ZabdZabac.\n",
    "4. The next common byte pair is \"ab\" with frequency 2. Replace \"ab\" with \"Y\". The new data becomes ZYdZYac.\n",
    "5. The next common byte pair is \"ZY\" with frequency 2. Replace \"ZY\" with \"W\". The new data becomes WdWac.\n",
    "\n",
    "Since we don't have any more pairs, we stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803c157",
   "metadata": {},
   "source": [
    "### How is BPE used in LLMs?\n",
    "BPE ensures that the most common words in the vocabulary are represented as single tokens, while less common words or rarer words are broken down into subword units. This allows the model to handle a wide range of vocabulary without needing to store every possible word in the vocabulary. \n",
    "\n",
    "Practical example:\n",
    "* Let's consider the below dataset of words -\n",
    "```\n",
    "{\"old\":7, \"older\":3, \"finest\":9, \"lowest\":4}\n",
    "```\n",
    "* **Preprocessing**: We need to add end token \"</w>\" at the end of each word. This is also done when training LLMs. The data now becomes -\n",
    "```\n",
    "{\"old</w>\":7, \"older</w>\":3, \"finest</w>\":9, \"lowest</w>\":4}\n",
    "```\n",
    "* Let us now split words into characters and count their frequency in the table below:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 23        |\n",
    "| o         | 14        |\n",
    "| l         | 14        |\n",
    "| d         | 10        |\n",
    "| e         | 16        |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| s         | 13        |\n",
    "| t         | 13        |\n",
    "| w         | 4         |\n",
    "\n",
    "* Now we will find the most frequent pairing of characters and **merge them** and perform the same iteration again and again until we reach teh token limit or iteration limit.\n",
    "\n",
    "* The most common pair in our dataset is (\"e\", \"s\") with frequency 13 (9 times in finest and 4 times in lowest). So we treat \"es\" as a single token. The new frequency table becomes:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 23        |\n",
    "| o         | 14        |\n",
    "| l         | 14        |\n",
    "| d         | 10        |\n",
    "| e         | 16-13 = 3 |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| s         | 13-13 = 0 |\n",
    "| t         | 13        |\n",
    "| w         | 4         |\n",
    "| es        | 9+4 = 13  |\n",
    "\n",
    "Now in the next iteration, the most common pair is (\"es\", \"t\") with frequency 13 (9 times in finest and 4 times in lowest). So we treat \"est\" as a single token. The new frequency table becomes:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 23        |\n",
    "| o         | 14        |\n",
    "| l         | 14        |\n",
    "| d         | 10        |\n",
    "| e         | 3         |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| t         | 13-13 = 0 |\n",
    "| w         | 4         |\n",
    "| es        | 13-13 = 0 |\n",
    "| est       | 9+4 = 13  |\n",
    "\n",
    "Again, the most common pair is (\"est\", \"</w>\") with frequency 13. So the table becomes:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 23-13 = 10|\n",
    "| o         | 14        |\n",
    "| l         | 14        |\n",
    "| d         | 10        |\n",
    "| e         | 3         |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| w         | 4         |\n",
    "| est       | 13-13 = 0 |\n",
    "| est\\</w\\> | 13        |\n",
    "\n",
    "If we didn't merge \"est\" and \"</w>\", there would be no difference between \"estimate\" and \"highest\". The tokenizer now knows that \"est\" is an ending sequence in all the words in our dataset, so we need to encode that information that it's an ending sequence.\n",
    "\n",
    "On running the 4th iteration, we find out that (\"o\", \"l\") is the most common pair with frequency 10. So we treat \"ol\" as a single token. The new frequency table becomes:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 10        |\n",
    "| o         | 14-10 = 4 |\n",
    "| l         | 14-10 = 4 |\n",
    "| d         | 10        |\n",
    "| e         | 3         |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| w         | 4         |\n",
    "| est\\</w\\> | 13        |\n",
    "| ol        | 7+3 = 10  |\n",
    "\n",
    "On the 5th iteration, we find out (\"ol\", \"d\") has frequency 10. The new table now becomes:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 10        |\n",
    "| o         | 4         |\n",
    "| l         | 4         |\n",
    "| d         | 10-10 = 0 |\n",
    "| e         | 3         |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| w         | 4         |\n",
    "| est\\</w\\> | 13        |\n",
    "| ol        | 10-10 = 0 |\n",
    "| old       | 7+3 = 10  |\n",
    "\n",
    "We can also observe that (\"f\", \"i\", \"n\") occur 9 times but that's just from a single word so we don't merge them. We also remove the tokens that have frequency 0. Finally, we have the following tokens:\n",
    "\n",
    "|   Token   | Frequency |\n",
    "|-----------|-----------|\n",
    "| \\</w\\>    | 10        |\n",
    "| o         | 4         |\n",
    "| l         | 4         |\n",
    "| e         | 3         |\n",
    "| r         | 3         |\n",
    "| f         | 9         |\n",
    "| i         | 9         |\n",
    "| n         | 9         |\n",
    "| w         | 4         |\n",
    "| est\\</w\\> | 13        |\n",
    "| old       | 10        |\n",
    "\n",
    "So the above tokens are the final list of tokens that will serve as our vocabulary. The BPE algorithm will continue to merge tokens until we reach the desired vocabulary size, or if we have a desired number of iterations for which it will run or until no more merges are possible.\n",
    "\n",
    "Since implementing BPE from scratch is a bit complex, we will use the `tiktoken` library, which is a fast BPE tokenizer used by OpenAI's GPT models. It's very efficient since it's source code is written in Rust and it has Python bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1384eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import importlib.metadata\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7ed7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faf98ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded integers: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\"Encoded integers:\", integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "488d1aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' someunknownPlace.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([617, 34680, 27271, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f293927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea88ac9",
   "metadata": {},
   "source": [
    "To handle words that are not in the vocabulary, the BPE algorithm breaks it down into familiar subwords and characters which are present in the vocabulary. This is how the OOV problem is solved using a subword tokenizer like BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b481c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94968aef",
   "metadata": {},
   "source": [
    "### Creating Input-Target Pairs\n",
    "\n",
    "LLMs are trained to predict the next token in a sequence given the previous tokens. To train an LLM, we need to create input-target pairs from our tokenized text. For example:\n",
    "\n",
    "**Our sentence**: LLMs learn to predict one word at a time\n",
    "\n",
    "**Input-Target Pair**: (LLMs, learn) -> (LLMs learn, to) -> (LLMs learn to, predict) -> (LLMs learn to predict, one) ... (LLMs learn to predict one word at a, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8329af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]  # Removing first 50 tokens from dataset to make results more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdce45",
   "metadata": {},
   "source": [
    "To create input-output pairs, we use the sliding window technique. We take a fixed-size window of tokens as input and the next token as output. The size of the window is called the context length. So, if our context length is 4 -\n",
    "\n",
    "- X (input): [1, 2, 3, 4]\n",
    "- Y (output): [2, 3, 4, 5]\n",
    "\n",
    "If X is 1, Y is 2. If X is [1, 2], Y is 3. If X is [1, 2, 3], Y is 4. If X is [1, 2, 3, 4], Y is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a444e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [290, 4920, 2241, 287]\n",
      "Y:       [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "X = enc_sample[:context_size]\n",
    "Y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(f\"X:  {X}\")\n",
    "print(f\"Y:       {Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "001e9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -> 4920\n",
      "[290, 4920] -> 2241\n",
      "[290, 4920, 2241] -> 287\n",
      "[290, 4920, 2241, 287] -> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59c7c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ->  established\n",
      " and established ->  himself\n",
      " and established himself ->  in\n",
      " and established himself in ->  a\n"
     ]
    }
   ],
   "source": [
    "# So if we convert this to text, we can see the inputs and targets of LLMs.\n",
    "for i in range(1, context_size+1):\n",
    "    context = tokenizer.decode(enc_sample[:i])\n",
    "    desired = tokenizer.decode([enc_sample[i]])\n",
    "    print(context, \"->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55129ae6",
   "metadata": {},
   "source": [
    "# Creating Input-Target Pairs with PyTorch Dataloaders\n",
    "\n",
    "We just need to perform this operation for the entire dataset to create input-target pairs. To do this we take the help of PyTorch Dataloaders, which will help us create batches of data for training. If our dataset has sample text:\n",
    "\n",
    "\"In the heart of the city stood the old library, a relic from a bygone era. It's stone walls bore the marks of time, and ivy clung to it's facade...\"\n",
    "\n",
    "Our input and output tensors look like -\n",
    "```\n",
    "x = ([[\"In\", \"the\", \"heart\", \"of\"],\n",
    "      [\"the\", \"city\", \"stood\", \"the\"],\n",
    "      [\"old\", \"library,\", \"a\", \"relic\"],\n",
    "      [...]])\n",
    "\n",
    "y = ([[\"the\", \"heart\", \"of\", \"the\"],\n",
    "      [\"city\", \"stood\", \"the\", \"old\"],\n",
    "      [\"library,\", \"a\", \"relic\", \"from\"],\n",
    "      [...]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ea642",
   "metadata": {},
   "source": [
    "- Step 1: Tokenize the entire text\n",
    "- Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "- Step 3: Return the total number of rows in the dataset\n",
    "- Step 4: Return a single row from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b969e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770e600",
   "metadata": {},
   "source": [
    "Now we are going to use PyTorch's DataLoader to create batches of data for training.\n",
    "- Step 1: Initialize the tokenizer\n",
    "- Step 2: Create dataset\n",
    "- Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "- Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1147d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48540d",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch_size of 1 for an LLM with a context size of 4.\n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/the-verdict.txt', 'r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df4cff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0\n",
      "First Batch:\n",
      " [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"First Batch:\\n\", first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843fa0a",
   "metadata": {},
   "source": [
    "Since the max_length is set to 4, each of the two tensors contain 4 token_IDs. Note that the input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of atleast 256 tokens.\n",
    "\n",
    "The meaning of stride - In the context of tokenization, stride refers to the step size taken when moving the sliding window across the input text. A larger stride results in fewer overlapping tokens, while a smaller stride increases overlap and potentially captures more context. More overlap can lead to overfitting, while less overlap can lead to underfitting. The choice of stride depends on the specific task and the desired balance between context and generalization.\n",
    "\n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c845e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Batch: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(\"Second Batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f49e5",
   "metadata": {},
   "source": [
    "Small batch sizes lead to faster training but noisier updates. Large batch sizes lead to slower training but more stable updates. The choice of batch size depends on the available memory and the specific task. It's a hyperparameter that we should carefully adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bef6ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfbb170",
   "metadata": {},
   "source": [
    "Here, we increase the stride to 4. Since max_length is also 4, this utilizes the dataset fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap may lead to increased overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09b54a",
   "metadata": {},
   "source": [
    "### What are Token Embeddings?\n",
    "\n",
    "We've already converted our text into Token IDs and we maintain a vocabulary that contains all the unique token IDs. This can be the input to our LLMs, so why do we need to create token embeddings? We cannot just use randomly assigned numbers. The problem with using random numbers:\n",
    "\n",
    "```\n",
    "\"cat\" -> 34\n",
    "\"book\" -> 2.9\n",
    "\"tablet\" -> -20\n",
    "\"kitten\" -> -13\n",
    "```\n",
    "\n",
    "\"cat\" and \"kitten\" are semantically related but the associated numbers 34 and -13 cannot capture this relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e8aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
